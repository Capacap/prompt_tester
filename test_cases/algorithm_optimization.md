This algorithm is too slow for large datasets. How can I optimize it?

```python
def find_duplicates(arr):
    duplicates = []
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j] and arr[i] not in duplicates:
                duplicates.append(arr[i])
    return duplicates
```

The function needs to handle arrays with up to 100,000 elements efficiently.
